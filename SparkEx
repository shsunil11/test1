lines = sc.textFile("spark/example1.txt")lines.filter(lambda x: "User02" in x).take(11)
lines.map(lambda x: x.upper()).take(11)
lines.flatMap(lambda x: x.split()).take(15)
lines.groupBy(lambda x: len(x)).take(11)
for group in lines.groupBy(lambda x: len(x)).collect():
   print "len = " + str(group[0])
   for element in group[1]:
      print "#   " + element
      
 
 
 
 cogroup
 ============
 rdd1 = sc.parallelize([(1,2),(3,4),(3,6)])
 rdd1 = sc.parallelize([(3,9)])
 cg = rdd1.cogroup(rdd2) 
 
 for elem in cg.take(11):
    print elem[0]
    print " ****************"
    for cols in elem[1]:
       y = ""
       for x in cols:
          y = y + str(x) + " "
       print y
       print "############"
       
       

      
 
   
  

