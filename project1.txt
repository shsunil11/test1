Project1

Goal :
Ingest tables from MySql retail_db into Hive
customers
categories
products
departments
order_detail (partitioned table created by joining orders and order_detail)

Step1.
=============================
Use the retail_db database in MySql
Create a daily partitioned table order_detail by joining orders and order_items.
The order_detail table will have an additional column trans_date of type date
Load approx 20% of records from order_items (joined with orders table) into each of the 5 partitions of the order_detail Table.

Step2.
=====================================
Create folder structure in local linux
/home/cloudera/training/project1
                                 /workflows/wf_db_ingest
                                 /hive  (ddl for hive tables)
                                 /scripts
                                 /logs
                                 
Create folder structure in HDFS
/user/cloudera/training/project1
                                 /workflows/wf_db_ingest
                        
Step3.
=====================================
Create hive db retail_stg with the requied tables 
Table structure should be same as that of MySql
Tables should be text with delimiter as control A and not compressed

The goal is to have the final table in parquet format (see Step 4).
However, we do not want to sqoop directly in parquet, since sqoop in text format is much faster.
So we sqoop as text and land in the hive staging table location, then use hive hql to load the text data from staging into the parquet table.

Step4.
====================================================
Create hive db retail.
order_detail should be partitioned on trans_date
Tables should be parquet tables compressed with snappy

                                 
  Step5.
  ============================================
  Create one parameterized oozie workflow with 3 actions
  sqoop - to sqoop from  MySql into retail_stg hive db  as delimited text with control A as delimiter
         - Handle nulls and also handle presence of the delimite or newline in the data 
         - Use password-alias feature of sqoop to provide MySql db password.
  hive2  - to load the sqooped data into a parquet table in retail database
         -  Use dynamic partitioning for partitioned tables
  shell action - python script to validate the counts between MySql and Impala (use invalidate metadata on impala table before querying it)
  
  Create a properties file for each table
  The sqoop action should be parameterized for table name, database name, column names etc
  The hive script to load from staging into parquet tables should also be parameterized for list of columns
  
  Step 6.
  =============================================================
  Create a python script to run a oozie workflow, should take the workflow name and path as parameters
  The script should submit the workflow using oozie command and loop until workflow completes of fails
  The script should log the progress of the workflow into the logs directory
  
  Step 7.
  =================================
  Ingest the tables using the oozie workflow
  For the order_detail table, ingest one partition, then next 2 partitions and then next 2 partitions
  
  
  
  

                                 
                                 
