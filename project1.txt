Project1

Goal :
Ingest tables from MySql retail_db into Hive
customers
categories
products
departments
order_detail (partitioned table created by joining orders and order_detail)

Step1.
=============================
Use the retail_db database in MySql
Create a daily partitioned table order_detail by joining orders and order_items.
The order_detail table will have and additional column trans_date of type date
Load approx 20% of records from order_items (joined with orders table) into each of the 5 partitions of the order_detail Table.

Step2.
=====================================
Create folder structure in local linux
/home/cloudera/training/project1
                                 /workflows/wf_db_ingest
                                 /hive
                                 /scripts
                                 /logs
                                 
Create folder structure in HDFS
/user/cloudera/training/project1
                                 /workflows/wf_db_ingest
                        
Step3.
=====================================
Create hive db retail_stg with the requied tables 
Table structure should be same as that of MySql
Tables should be text with delimiter as control A

Step4.
====================================================
Create hive db retail.
order_detail should be partitioned on trans_date
Tables should be parquet tables compressed with snappy

                                 
  Step5.
  ============================================
  Create one parameterized oozie workflow with 3 actions
  sqoop - to sqoop from  MySql into retail_stg db  as delimited text with control A as delimiter
         - Handle nulls and also handle presence of the delimite or newline in the data 
  hive2  - to load the sqooped data into a parquet table in retail database
         -  Use dynamic partitioning for partitioned tables
  python script to validate the counts between MySql and Impala
  
  Create a properties file for each table
  The sqoop action should be parameterized for table name, database name, column name etc
  The hive script to load from staging into parquet tables should also be parameterized for list of columns
  
  Step 6.
  =============================================================
  Create a python script to run a oozie workflow, should take the workflow name and path ad parameters
  Should submit the workflow using oozie and loop until workflow completes of fails
  Should log the progress of the workflow into the logs directory
  
  Step 7.
  =================================
  Ingest the tables using the oozie workflow
  For the order_detail table, ingest one partition, then next 2 partitions and then next 2 partitions
  
  
  
  

                                 
                                 
